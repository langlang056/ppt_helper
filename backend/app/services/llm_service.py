"""Gemini LLM æœåŠ¡"""
import google.generativeai as genai
from app.config import get_settings
from PIL import Image
from typing import List, Optional, AsyncGenerator
from dataclasses import dataclass
import asyncio

settings = get_settings()

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
SUPPORTED_MODELS = [
    "gemini-2.5-flash",
    "gemini-2.5-pro",
]

# å®‰å…¨è®¾ç½®ï¼šç¦ç”¨æ‰€æœ‰è¿‡æ»¤å™¨ï¼ˆå­¦æœ¯å†…å®¹ï¼‰
SAFETY_SETTINGS = [
    {"category": cat, "threshold": "BLOCK_NONE"}
    for cat in [
        "HARM_CATEGORY_HARASSMENT",
        "HARM_CATEGORY_HATE_SPEECH",
        "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "HARM_CATEGORY_DANGEROUS_CONTENT",
    ]
]

# æ”¹è¿›çš„æç¤ºè¯æ¨¡æ¿ï¼ˆå‚è€ƒè€é¡¹ç›®ï¼‰
DEFAULT_PROMPT_TEMPLATE = """è¯·ä½œä¸ºä¸€åä¸“ä¸šçš„æ•™å¸ˆ,è¯¦ç»†åˆ†æè¿™ä¸€é¡µè¯¾ä»¶çš„å†…å®¹ã€‚

è¯·åŒ…æ‹¬ä»¥ä¸‹å†…å®¹:
1. **ä¸»é¢˜æ¦‚è¿°**: è¿™ä¸€é¡µçš„ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆ?
2. **æ ¸å¿ƒæ¦‚å¿µ**: åˆ—å‡ºå¹¶è§£é‡Šé¡µé¢ä¸Šçš„å…³é”®æ¦‚å¿µã€å®šä¹‰æˆ–æœ¯è¯­
3. **å…¬å¼å’Œå›¾è¡¨**: å¦‚æœæœ‰æ•°å­¦å…¬å¼ã€å›¾è¡¨æˆ–å›¾ç¤º,è¯·è¯¦ç»†è§£é‡Šå®ƒä»¬çš„å«ä¹‰
4. **é‡ç‚¹éš¾ç‚¹**: æŒ‡å‡ºè¿™ä¸€é¡µä¸­å­¦ç”Ÿå¯èƒ½éš¾ä»¥ç†è§£çš„éƒ¨åˆ†
5. **çŸ¥è¯†ç‚¹æ€»ç»“**: ç”¨ç®€æ´çš„è¯­è¨€æ€»ç»“è¿™ä¸€é¡µçš„è¦ç‚¹
6. **ä¸å‰æ–‡è”ç³»**: å¦‚æœæä¾›äº†å‰é¢é¡µé¢çš„ä¿¡æ¯,è¯·è¯´æ˜è¿™ä¸€é¡µå¦‚ä½•æ‰¿æ¥æˆ–æ·±åŒ–å‰é¢çš„å†…å®¹

**é‡è¦æ ¼å¼è¦æ±‚**:
- ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡º
- æ‰€æœ‰æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX è¯­æ³•:
  - è¡Œå†…å…¬å¼ä½¿ç”¨å•ä¸ªç¾å…ƒç¬¦å·åŒ…è£¹,å¦‚ $E = mc^2$
  - å—çº§å…¬å¼(ç‹¬ç«‹æˆè¡Œçš„å…¬å¼)å¿…é¡»ä½¿ç”¨åŒç¾å…ƒç¬¦å·åŒ…è£¹,å¦‚:
    $$F(x) = \\int_{-\\infty}^{\\infty} f(t) dt$$
  - å¤šè¡Œå…¬å¼ä¹Ÿå¿…é¡»ç”¨åŒç¾å…ƒç¬¦å·åŒ…è£¹,å¦‚:
    $$
    \\begin{aligned}
    a &= b + c \\\\
    d &= e + f
    \\end{aligned}
    $$
- ç»å¯¹ä¸è¦ç›´æ¥å†™ \\begin{align*} æˆ– \\begin{equation} è€Œä¸ç”¨ $$ åŒ…è£¹
- ä¸è¦ä½¿ç”¨ Unicode æ•°å­¦ç¬¦å·(å¦‚ Î£ã€âˆ«ã€âˆš),å¿…é¡»ä½¿ç”¨ LaTeX å‘½ä»¤(å¦‚ \\sumã€\\intã€\\sqrt)
- ä¸Šä¸‹æ ‡å¿…é¡»ç”¨ LaTeX è¯­æ³•,å¦‚ $x_i$ã€$x^2$,è€Œä¸æ˜¯ xáµ¢ã€xÂ²

è¯·ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”,å°±åƒåœ¨ç»™å­¦ç”Ÿè®²è§£ä¸€æ ·ã€‚"""


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    api_key: str
    model: str = "gemini-2.5-flash"


class GeminiService:
    """Gemini Vision æœåŠ¡ - æ”¯æŒåŠ¨æ€é…ç½®"""

    def __init__(self, config: Optional[LLMConfig] = None):
        """
        åˆå§‹åŒ– Gemini æœåŠ¡

        Args:
            config: LLM é…ç½®ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        self.prompt_template = DEFAULT_PROMPT_TEMPLATE
        self._model = None
        self._api_key = None
        self._model_name = None

        if config:
            self._init_with_config(config)
        elif settings.google_api_key:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
            self._init_with_config(LLMConfig(
                api_key=settings.google_api_key,
                model=settings.google_model
            ))
            print(f"âœ… Gemini ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®: {settings.google_model}")
        else:
            print("âš ï¸ Gemini æœªé…ç½®ï¼Œç­‰å¾…å®¢æˆ·ç«¯æä¾› API Key")

    def _init_with_config(self, config: LLMConfig):
        """ä½¿ç”¨é…ç½®åˆå§‹åŒ–"""
        if not config.api_key:
            raise ValueError("API Key æœªæä¾›")

        # éªŒè¯æ¨¡å‹
        if config.model not in SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {config.model}ï¼Œæ”¯æŒ: {SUPPORTED_MODELS}")

        genai.configure(api_key=config.api_key)
        self._model = genai.GenerativeModel(config.model)
        self._api_key = config.api_key
        self._model_name = config.model

    def configure(self, config: LLMConfig):
        """åŠ¨æ€æ›´æ–°é…ç½®"""
        self._init_with_config(config)
        print(f"âœ… Gemini é…ç½®å·²æ›´æ–°: {config.model}")

    @property
    def model(self):
        if self._model is None:
            raise ValueError("Gemini æœªé…ç½®ï¼Œè¯·å…ˆæä¾› API Key")
        return self._model

    @property
    def is_configured(self) -> bool:
        return self._model is not None

    def extract_summary(self, analysis_text: str, page_num: int) -> str:
        """
        ä»åˆ†æç»“æœä¸­æå–å…³é”®æ‘˜è¦
        
        Args:
            analysis_text: å®Œæ•´çš„åˆ†ææ–‡æœ¬
            page_num: é¡µç 
            
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # æå–å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        lines = analysis_text.split('\n')
        summary_lines = []
        char_count = 0
        
        for line in lines:
            if char_count > 200:
                break
            if line.strip() and not line.startswith('#'):
                summary_lines.append(line.strip())
                char_count += len(line)
        
        summary = ' '.join(summary_lines)[:200]
        return f"[ç¬¬{page_num}é¡µæ‘˜è¦] {summary}"

    def build_context_string(self, previous_summaries: List[str]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not previous_summaries:
            return ""
        
        context = "\n".join(previous_summaries)
        return f"\n\nğŸ“š å‰é¢é¡µé¢çš„å†…å®¹æ¦‚è¦:\n{context}\n"

    async def analyze_image(
        self,
        image: Image.Image,
        page_num: int,
        previous_summaries: Optional[List[str]] = None,
        temperature: float = 0.7,
        max_tokens: int = 50000,
    ) -> str:
        """åˆ†æå›¾åƒå¹¶ç”ŸæˆMarkdownæ ¼å¼è§£é‡Š"""
        
        # æ„å»ºæç¤ºè¯
        prompt = f"ã€ç¬¬ {page_num} é¡µã€‘\n\n{self.prompt_template}"
        
        # æ·»åŠ å‰é¢é¡µé¢çš„ä¸Šä¸‹æ–‡
        if previous_summaries:
            context_str = self.build_context_string(previous_summaries)
            prompt += context_str

        # ç”Ÿæˆé…ç½® - å¢åŠ  max_output_tokens
        config = genai.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
        )

        # é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(
                    [prompt, image],
                    generation_config=config,
                    safety_settings=SAFETY_SETTINGS,
                )

                # æ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰å“åº”
                if not response.candidates:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šæ— å€™é€‰å“åº”ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        continue
                    return f"## ç¬¬ {page_num} é¡µ\n\nâš ï¸ æ— æ³•ç”Ÿæˆå†…å®¹ï¼Œè¯·ç¨åé‡è¯•ã€‚"

                candidate = response.candidates[0]

                # è®°å½•å®‰å…¨æ ‡è®°
                if candidate.finish_reason == 2:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šSAFETY æ ‡è®°")
                elif candidate.finish_reason == 3:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šRECITATION æ ‡è®°")

                # å°è¯•å¤šç§æ–¹å¼æå–æ–‡æœ¬
                extracted_text = None
                
                # æ–¹å¼1: ç›´æ¥ä» response.text è·å–
                try:
                    if hasattr(response, "text") and response.text:
                        extracted_text = response.text
                except ValueError as e:
                    # response.text å¯èƒ½å› ä¸ºå®‰å…¨åŸå› æŠ›å‡ºå¼‚å¸¸
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šresponse.text å¼‚å¸¸: {str(e)[:100]}")

                # æ–¹å¼2: ä» candidate.content.parts æå–
                if not extracted_text and candidate.content and candidate.content.parts:
                    texts = []
                    for part in candidate.content.parts:
                        if hasattr(part, "text") and part.text:
                            texts.append(part.text)
                    if texts:
                        extracted_text = "\n".join(texts)

                # æ–¹å¼3: å°è¯•ä» candidate çš„å…¶ä»–å±æ€§æå–
                if not extracted_text:
                    try:
                        if hasattr(candidate, 'text') and candidate.text:
                            extracted_text = candidate.text
                    except:
                        pass

                if extracted_text and len(extracted_text.strip()) > 50:
                    return extracted_text
                elif extracted_text:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šå†…å®¹è¿‡çŸ­ ({len(extracted_text)} å­—ç¬¦)ï¼Œé‡è¯•")
                    if attempt < max_retries - 1:
                        continue
                    return extracted_text if extracted_text else f"## ç¬¬ {page_num} é¡µ\n\nâš ï¸ å†…å®¹ç”Ÿæˆä¸å®Œæ•´ã€‚"
                else:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šæ— æ³•æå–å†…å®¹ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        continue
                    return f"## ç¬¬ {page_num} é¡µ\n\nâš ï¸ æ— æ³•æå–å†…å®¹ï¼Œå¯èƒ½æ˜¯å®‰å…¨è¿‡æ»¤å¯¼è‡´ã€‚"

            except Exception as e:
                print(f"âš ï¸ ç¬¬ {page_num} é¡µï¼šGemini API é”™è¯¯: {str(e)}")
                if attempt < max_retries - 1:
                    import asyncio
                    await asyncio.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    continue
                return f"## ç¬¬ {page_num} é¡µ\n\nâš ï¸ ç”Ÿæˆå¤±è´¥: {str(e)[:200]}"
        
        return f"## ç¬¬ {page_num} é¡µ\n\nâš ï¸ å¤šæ¬¡å°è¯•åä»æ— æ³•ç”Ÿæˆå†…å®¹ã€‚"


    async def chat_stream(
        self,
        question: str,
        context: str,
        history: List[dict],
        page_number: int,
        temperature: float = 0.7,
        max_tokens: int = 50000,
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼èŠå¤©å“åº”

        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: å½“å‰é¡µé¢çš„è§£é‡Šå†…å®¹ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
            history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            page_number: å½“å‰é¡µç 
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°

        Yields:
            æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯¾ä»¶è®²è§£åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨é˜…è¯»ç¬¬ {page_number} é¡µçš„è¯¾ä»¶å†…å®¹ï¼Œä¸‹é¢æ˜¯è¯¥é¡µçš„è¯¦ç»†è®²è§£ï¼š

---
{context if context else "ï¼ˆè¯¥é¡µæš‚æ— è®²è§£å†…å®¹ï¼‰"}
---

è¯·åŸºäºä»¥ä¸Šå†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœé—®é¢˜ä¸å½“å‰é¡µé¢å†…å®¹ç›¸å…³ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç»™å‡ºè¯¦ç»†è§£ç­”ã€‚
å¦‚æœé—®é¢˜è¶…å‡ºå½“å‰é¡µé¢èŒƒå›´ï¼Œå¯ä»¥æ ¹æ®ä½ çš„çŸ¥è¯†è¿›è¡Œè¡¥å……ï¼Œä½†è¯·è¯´æ˜è¿™æ˜¯é¢å¤–è¡¥å……çš„å†…å®¹ã€‚
è¯·ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ï¼Œå¯ä»¥ä½¿ç”¨ Markdown æ ¼å¼ã€‚"""

        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []

        # æ·»åŠ å†å²æ¶ˆæ¯
        for msg in history:
            role = "user" if msg["role"] == "user" else "model"
            messages.append({"role": role, "parts": [msg["content"]]})

        # æ·»åŠ å½“å‰é—®é¢˜
        messages.append({"role": "user", "parts": [question]})

        # ç”Ÿæˆé…ç½®
        config = genai.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
        )

        try:
            # ä½¿ç”¨ chat æ¨¡å¼
            chat = self.model.start_chat(history=messages[:-1] if messages[:-1] else [])

            # æµå¼ç”Ÿæˆ
            response = chat.send_message(
                f"{system_prompt}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}",
                generation_config=config,
                safety_settings=SAFETY_SETTINGS,
                stream=True,
            )

            for chunk in response:
                if chunk.text:
                    yield chunk.text
                    await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ

        except Exception as e:
            print(f"âŒ èŠå¤©æµå¼å“åº”é”™è¯¯: {str(e)}")
            yield f"\n\næŠ±æ­‰ï¼Œå‘ç”Ÿé”™è¯¯ï¼š{str(e)}"


# å…¨å±€å•ä¾‹ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼‰
# å¦‚æœç¯å¢ƒå˜é‡é…ç½®äº† API Keyï¼Œåˆ™åˆ›å»ºé»˜è®¤å®ä¾‹
# å¦åˆ™åˆ›å»ºæœªé…ç½®çš„å®ä¾‹ï¼Œç­‰å¾…å®¢æˆ·ç«¯æä¾›é…ç½®
llm_service = GeminiService()


def create_llm_service(api_key: str, model: str = "gemini-2.5-flash") -> GeminiService:
    """
    åˆ›å»ºæ–°çš„ LLM æœåŠ¡å®ä¾‹ï¼ˆä½¿ç”¨å®¢æˆ·ç«¯é…ç½®ï¼‰

    Args:
        api_key: Google API Key
        model: æ¨¡å‹åç§°

    Returns:
        é…ç½®å¥½çš„ GeminiService å®ä¾‹
    """
    config = LLMConfig(api_key=api_key, model=model)
    return GeminiService(config)
